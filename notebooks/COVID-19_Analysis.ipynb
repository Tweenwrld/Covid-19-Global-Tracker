{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a54f894",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading COVID-19 data from Our World in Data...\n"
     ]
    },
    {
     "ename": "IncompleteRead",
     "evalue": "IncompleteRead(68898725 bytes read, 33990706 more expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIncompleteRead\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading COVID-19 data from Our World in Data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://covid.ourworldindata.org/data/owid-covid-data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Display basic info\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:728\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    725\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    727\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 728\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m _get_filepath_or_buffer(\n\u001b[0;32m    729\u001b[0m     path_or_buf,\n\u001b[0;32m    730\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    731\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    732\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m    733\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    734\u001b[0m )\n\u001b[0;32m    736\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    737\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:389\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m content_encoding \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;66;03m# Override compression based on Content-Encoding header\u001b[39;00m\n\u001b[0;32m    388\u001b[0m             compression \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgzip\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m--> 389\u001b[0m         reader \u001b[38;5;241m=\u001b[39m BytesIO(req\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m IOArgs(\n\u001b[0;32m    391\u001b[0m         filepath_or_buffer\u001b[38;5;241m=\u001b[39mreader,\n\u001b[0;32m    392\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    395\u001b[0m         mode\u001b[38;5;241m=\u001b[39mfsspec_mode,\n\u001b[0;32m    396\u001b[0m     )\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_fsspec_url(filepath_or_buffer):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:495\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 495\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_safe_read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength)\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m IncompleteRead:\n\u001b[0;32m    497\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\http\\client.py:642\u001b[0m, in \u001b[0;36mHTTPResponse._safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    640\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m<\u001b[39m amt:\n\u001b[1;32m--> 642\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(data, amt\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mIncompleteRead\u001b[0m: IncompleteRead(68898725 bytes read, 33990706 more expected)"
     ]
    }
   ],
   "source": [
    "# ====== CLUSTER ANALYSIS: PANDEMIC RESPONSE PATTERNS ======\n",
    "print(\"\\n=== CLUSTER ANALYSIS: PANDEMIC RESPONSE PATTERNS ===\")\n",
    "\n",
    "# We'll use K-means clustering to identify groups of countries with similar pandemic patterns\n",
    "# First, prepare features for clustering\n",
    "\n",
    "# Define a function to extract relevant features for each country\n",
    "def extract_country_features(country_name):\n",
    "    \"\"\"Extract pandemic response features for a given country\"\"\"\n",
    "    country_data = df[df['location'] == country_name].copy()\n",
    "    \n",
    "    # Skip if not enough data\n",
    "    if len(country_data) < 30:\n",
    "        return None\n",
    "    \n",
    "    # Get the max total cases and deaths\n",
    "    max_cases_per_million = country_data['cases_per_million'].max() if 'cases_per_million' in country_data.columns else np.nan\n",
    "    max_deaths_per_million = country_data['deaths_per_million'].max() if 'deaths_per_million' in country_data.columns else np.nan\n",
    "    \n",
    "    # Calculate case fatality rate\n",
    "    cfr = (country_data['total_deaths'].max() / country_data['total_cases'].max() * 100) if country_data['total_cases'].max() > 0 else np.nan\n",
    "    \n",
    "    # Calculate average stringency if available\n",
    "    avg_stringency = country_data['stringency_index'].mean() if 'stringency_index' in country_data.columns else np.nan\n",
    "    \n",
    "    # Calculate vaccination rate if available\n",
    "    max_vax_rate = country_data['people_fully_vaccinated_per_hundred'].max() if 'people_fully_vaccinated_per_hundred' in country_data.columns else np.nan\n",
    "    \n",
    "    # Get population density and median age if available\n",
    "    pop_density = country_data['population_density'].iloc[0] if 'population_density' in country_data.columns else np.nan\n",
    "    median_age = country_data['median_age'].iloc[0] if 'median_age' in country_data.columns else np.nan\n",
    "    \n",
    "    # Calculate peak intensity (max new cases per million)\n",
    "    peak_intensity = country_data['new_cases_per_million'].max() if 'new_cases_per_million' in country_data.columns else np.nan\n",
    "    \n",
    "    # Wave count (using our previous wave detection logic)\n",
    "    wave_count = np.nan\n",
    "    if 'cases_7day_avg' in country_data.columns:\n",
    "        smooth_cases = country_data['cases_7day_avg'].fillna(0).values\n",
    "        if len(smooth_cases) > 0 and np.max(smooth_cases) > 0:\n",
    "            peaks, _ = find_peaks(\n",
    "                smooth_cases, \n",
    "                prominence=np.max(smooth_cases) * 0.1,\n",
    "                distance=14\n",
    "            )\n",
    "            wave_count = len(peaks)\n",
    "    \n",
    "    return {\n",
    "        'country': country_name,\n",
    "        'cases_per_million': max_cases_per_million,\n",
    "        'deaths_per_million': max_deaths_per_million,\n",
    "        'case_fatality_rate': cfr,\n",
    "        'stringency_index': avg_stringency,\n",
    "        'vaccination_rate': max_vax_rate,\n",
    "        'population_density': pop_density,\n",
    "        'median_age': median_age,\n",
    "        'peak_intensity': peak_intensity,\n",
    "        'wave_count': wave_count\n",
    "    }\n",
    "\n",
    "# Apply the function to each country\n",
    "print(\"Extracting features for clustering analysis...\")\n",
    "countries = df['location'].unique()\n",
    "features_list = []\n",
    "\n",
    "for country in countries:\n",
    "    # Skip continents or regions\n",
    "    if country in ['World', 'International', 'Europe', 'Asia', 'North America', 'South America', 'Africa', 'Oceania']:\n",
    "        continue\n",
    "    \n",
    "    features = extract_country_features(country)\n",
    "    if features is not None:\n",
    "        features_list.append(features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "features_df = pd.DataFrame(features_list)\n",
    "\n",
    "# Check if we have enough data for clustering\n",
    "if len(features_df) > 10:\n",
    "    print(f\"Collected features for {len(features_df)} countries\")\n",
    "    \n",
    "    # Select features for clustering, dropping rows with missing values\n",
    "    cluster_features = ['cases_per_million', 'deaths_per_million', 'case_fatality_rate', \n",
    "                         'stringency_index', 'peak_intensity', 'wave_count']\n",
    "    \n",
    "    # Keep only features that are mostly available\n",
    "    available_features = [col for col in cluster_features if features_df[col].notna().mean() >= 0.7]\n",
    "    \n",
    "    if len(available_features) >= 3:  # Need at least 3 features for meaningful clustering\n",
    "        print(f\"Using features for clustering: {', '.join(available_features)}\")\n",
    "        \n",
    "        # Drop rows with missing values in the selected features\n",
    "        cluster_data = features_df.dropna(subset=available_features)\n",
    "        \n",
    "        if len(cluster_data) >= 30:  # Need at least 30 countries for meaningful clustering\n",
    "            print(f\"Clustering {len(cluster_data)} countries with complete data\")\n",
    "            \n",
    "            # Standardize the features\n",
    "            scaler = StandardScaler()\n",
    "            scaled_features = scaler.fit_transform(cluster_data[available_features])\n",
    "            \n",
    "            # Determine optimal number of clusters using silhouette score\n",
    "            silhouette_scores = []\n",
    "            max_clusters = min(8, len(cluster_data) // 5)  # Cap at 8 clusters or 1/5 of data points\n",
    "            \n",
    "            for k in range(2, max_clusters + 1):\n",
    "                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "                cluster_labels = kmeans.fit_predict(scaled_features)\n",
    "                silhouette_avg = silhouette_score(scaled_features, cluster_labels)\n",
    "                silhouette_scores.append(silhouette_avg)\n",
    "            \n",
    "            # Find optimal K\n",
    "            optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2  # +2 because we started at 2\n",
    "            print(f\"Optimal number of clusters identified: {optimal_k}\")\n",
    "            \n",
    "            # Perform K-means clustering with optimal K\n",
    "            kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "            cluster_data['cluster'] = kmeans.fit_predict(scaled_features)\n",
    "            \n",
    "            # Analyze clusters\n",
    "            cluster_stats = cluster_data.groupby('cluster').agg({\n",
    "                'country': 'count',\n",
    "                'cases_per_million': 'mean',\n",
    "                'deaths_per_million': 'mean',\n",
    "                'case_fatality_rate': 'mean',\n",
    "                'stringency_index': 'mean' if 'stringency_index' in available_features else 'count',\n",
    "                'peak_intensity': 'mean' if 'peak_intensity' in available_features else 'count',\n",
    "                'wave_count': 'mean' if 'wave_count' in available_features else 'count'\n",
    "            }).rename(columns={'country': 'count'})\n",
    "            \n",
    "            print(\"\\nCluster analysis results:\")\n",
    "            print(cluster_stats.round(2))\n",
    "            \n",
    "            # Interpretation of clusters\n",
    "            print(\"\\nCluster interpretation:\")\n",
    "            for cluster_id in range(optimal_k):\n",
    "                cluster_countries = cluster_data[cluster_data['cluster'] == cluster_id]['country'].tolist()\n",
    "                sample_countries = ', '.join(cluster_countries[:5])\n",
    "                if len(cluster_countries) > 5:\n",
    "                    sample_countries += f\" and {len(cluster_countries) - 5} more\"\n",
    "                    \n",
    "                print(f\"\\nCluster {cluster_id+1} ({len(cluster_countries)} countries, including {sample_countries}):\")\n",
    "                \n",
    "                # Describe this cluster's characteristics\n",
    "                characteristics = []\n",
    "                \n",
    "                cases = cluster_stats.loc[cluster_id, 'cases_per_million']\n",
    "                deaths = cluster_stats.loc[cluster_id, 'deaths_per_million']\n",
    "                cfr = cluster_stats.loc[cluster_id, 'case_fatality_rate']\n",
    "                \n",
    "                # Cases assessment\n",
    "                if cases < cluster_stats['cases_per_million'].quantile(0.33):\n",
    "                    characteristics.append(\"LOW case rates\")\n",
    "                elif cases > cluster_stats['cases_per_million'].quantile(0.67):\n",
    "                    characteristics.append(\"HIGH case rates\")\n",
    "                else:\n",
    "                    characteristics.append(\"MODERATE case rates\")\n",
    "                    \n",
    "                # Deaths assessment\n",
    "                if deaths < cluster_stats['deaths_per_million'].quantile(0.33):\n",
    "                    characteristics.append(\"LOW death rates\")\n",
    "                elif deaths > cluster_stats['deaths_per_million'].quantile(0.67):\n",
    "                    characteristics.append(\"HIGH death rates\")\n",
    "                else:\n",
    "                    characteristics.append(\"MODERATE death rates\")\n",
    "                    \n",
    "                # CFR assessment\n",
    "                if cfr < cluster_stats['case_fatality_rate'].quantile(0.33):\n",
    "                    characteristics.append(\"LOW case fatality\")\n",
    "                elif cfr > cluster_stats['case_fatality_rate'].quantile(0.67):\n",
    "                    characteristics.append(\"HIGH case fatality\")\n",
    "                else:\n",
    "                    characteristics.append(\"MODERATE case fatality\")\n",
    "                \n",
    "                # Add more characteristics if data available\n",
    "                if 'stringency_index' in available_features:\n",
    "                    stringency = cluster_stats.loc[cluster_id, 'stringency_index']\n",
    "                    if stringency < cluster_stats['stringency_index'].quantile(0.33):\n",
    "                        characteristics.append(\"LENIENT policies\")\n",
    "                    elif stringency > cluster_stats['stringency_index'].quantile(0.67):\n",
    "                        characteristics.append(\"STRICT policies\")\n",
    "                    else:\n",
    "                        characteristics.append(\"MODERATE policies\")\n",
    "                \n",
    "                if 'wave_count' in available_features:\n",
    "                    waves = cluster_stats.loc[cluster_id, 'wave_count']\n",
    "                    if waves < cluster_stats['wave_count'].quantile(0.33):\n",
    "                        characteristics.append(\"FEW waves\")\n",
    "                    elif waves > cluster_stats['wave_count'].quantile(0.67):\n",
    "                        characteristics.append(\"MANY waves\")\n",
    "                    else:\n",
    "                        characteristics.append(\"AVERAGE number of waves\")\n",
    "                \n",
    "                print(f\"  Characteristics: {', '.join(characteristics)}\")\n",
    "            \n",
    "            # Visualize clusters using PCA for dimensionality reduction\n",
    "            from sklearn.decomposition import PCA\n",
    "            \n",
    "            # Apply PCA to reduce to 2 dimensions for visualization\n",
    "            pca = PCA(n_components=2)\n",
    "            principal_components = pca.fit_transform(scaled_features)\n",
    "            \n",
    "            # Create a DataFrame for visualization\n",
    "            pca_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "            pca_df['cluster'] = cluster_data['cluster']\n",
    "            pca_df['country'] = cluster_data['country']\n",
    "            \n",
    "            # Create a scatter plot\n",
    "            fig_clusters = px.scatter(\n",
    "                pca_df, \n",
    "                x='PC1', \n",
    "                y='PC2', \n",
    "                color='cluster',\n",
    "                hover_name='country',\n",
    "                title='Country Clustering Based on COVID-19 Response Patterns',\n",
    "                labels={'PC1': 'Principal Component 1', 'PC2': 'Principal Component 2'},\n",
    "                color_continuous_scale=px.colors.qualitative.G10\n",
    "            )\n",
    "            \n",
    "            fig_clusters.update_layout(\n",
    "                xaxis_title=\"Principal Component 1\",\n",
    "                yaxis_title=\"Principal Component 2\"\n",
    "            )\n",
    "            \n",
    "            fig_clusters.show()\n",
    "            \n",
    "            # Create a 3D visualization if we have enough features\n",
    "            if len(available_features) >= 3:\n",
    "                # Select 3 important features for 3D visualization\n",
    "                selected_features = available_features[:3]\n",
    "                \n",
    "                fig_3d = px.scatter_3d(\n",
    "                    cluster_data, \n",
    "                    x=selected_features[0],\n",
    "                    y=selected_features[1],\n",
    "                    z=selected_features[2],\n",
    "                    color='cluster',\n",
    "                    hover_name='country',\n",
    "                    title=f'Country Clustering in 3D Feature Space',\n",
    "                    labels={\n",
    "                        selected_features[0]: selected_features[0].replace('_', ' ').title(),\n",
    "                        selected_features[1]: selected_features[1].replace('_', ' ').title(),\n",
    "                        selected_features[2]: selected_features[2].replace('_', ' ').title()\n",
    "                    },\n",
    "                    color_continuous_scale=px.colors.qualitative.G10\n",
    "                )\n",
    "                \n",
    "                fig_3d.update_layout(\n",
    "                    scene=dict(\n",
    "                        xaxis_title=selected_features[0].replace('_', ' ').title(),\n",
    "                        yaxis_title=selected_features[1].replace('_', ' ').title(),\n",
    "                        zaxis_title=selected_features[2].replace('_', ' ').title()\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                fig_3d.show()\n",
    "        else:\n",
    "            print(f\"Not enough countries ({len(cluster_data)}) with complete data for clustering\")\n",
    "    else:\n",
    "        print(f\"Not enough features available for clustering (need at least 3, found {len(available_features)})\")\n",
    "else:\n",
    "    print(\"Not enough countries with sufficient data for clustering analysis\")\n",
    "\n",
    "# ====== DEMOGRAPHIC IMPACT ANALYSIS ======\n",
    "print(\"\\n=== DEMOGRAPHIC IMPACT ANALYSIS ===\")\n",
    "\n",
    "# Check if we have demographic data\n",
    "demographic_columns = ['median_age', 'aged_65_older', 'aged_70_older', 'gdp_per_capita', \n",
    "                        'extreme_poverty', 'cardiovasc_death_rate', 'diabetes_prevalence',\n",
    "                        'life_expectancy', 'human_development_index']\n",
    "\n",
    "available_demographics = [col for col in demographic_columns if col in df.columns]\n",
    "\n",
    "if available_demographics:\n",
    "    print(f\"Available demographic indicators: {', '.join(available_demographics)}\")\n",
    "    \n",
    "    # Prepare data for correlation analysis\n",
    "    # We'll use the latest data for each country\n",
    "    latest_data = df.sort_values('date').groupby('location').last().reset_index()\n",
    "    \n",
    "    # Filter for countries with significant case counts\n",
    "    demo_analysis = latest_data[latest_data['total_cases'] >= 10000].copy()\n",
    "    \n",
    "    if len(demo_analysis) >= 30:  # Need reasonable sample size\n",
    "        print(f\"Analyzing demographic factors for {len(demo_analysis)} countries with significant outbreaks\")\n",
    "        \n",
    "        # Calculate correlation with deaths per million and CFR\n",
    "        if 'deaths_per_million' in demo_analysis.columns:\n",
    "            correlation_results = []\n",
    "            \n",
    "            for demo_col in available_demographics:\n",
    "                if demo_col in demo_analysis.columns:\n",
    "                    # Calculate correlation with deaths per million\n",
    "                    corr_deaths = demo_analysis[demo_col].corr(demo_analysis['deaths_per_million'])\n",
    "                    \n",
    "                    # Calculate correlation with CFR if available\n",
    "                    if 'case_fatality_rate' in demo_analysis.columns:\n",
    "                        corr_cfr = demo_analysis[demo_col].corr(demo_analysis['case_fatality_rate'])\n",
    "                    else:\n",
    "                        corr_cfr = np.nan\n",
    "                    \n",
    "                    correlation_results.append({\n",
    "                        'Demographic Factor': demo_col.replace('_', ' ').title(),\n",
    "                        'Correlation with Deaths/Million': corr_deaths,\n",
    "                        'Correlation with CFR': corr_cfr\n",
    "                    })\n",
    "            \n",
    "            # Convert to DataFrame and sort by correlation strength\n",
    "            corr_df = pd.DataFrame(correlation_results)\n",
    "            corr_df = corr_df.sort_values('Correlation with Deaths/Million', key=abs, ascending=False)\n",
    "            \n",
    "            print(\"\\nCorrelation of demographic factors with COVID-19 impact:\")\n",
    "            print(corr_df.round(3))\n",
    "            \n",
    "            # Create visualization for the top correlations\n",
    "            top_demos = corr_df.iloc[:5]['Demographic Factor'].tolist()\n",
    "            fig_corr = go.Figure()\n",
    "            \n",
    "            # Add bars for deaths per million correlation\n",
    "            fig_corr.add_trace(go.Bar(\n",
    "                x=corr_df['Demographic Factor'],\n",
    "                y=corr_df['Correlation with Deaths/Million'],\n",
    "                name='Correlation with Deaths/Million',\n",
    "                marker_color='darkred'\n",
    "            ))\n",
    "            \n",
    "            # Add bars for CFR correlation if available\n",
    "            if not corr_df['Correlation with CFR'].isna().all():\n",
    "                fig_corr.add_trace(go.Bar(\n",
    "                    x=corr_df['Demographic Factor'],\n",
    "                    y=corr_df['Correlation with CFR'],\n",
    "                    name='Correlation with Case Fatality Rate',\n",
    "                    marker_color='darkblue'\n",
    "                ))\n",
    "            \n",
    "            fig_corr.update_layout(\n",
    "                title='Correlation of Demographic Factors with COVID-19 Severity',\n",
    "                xaxis_title='Demographic Factor',\n",
    "                yaxis_title='Correlation Coefficient',\n",
    "                barmode='group',\n",
    "                xaxis={'categoryorder':'total descending'}\n",
    "            )\n",
    "            \n",
    "            fig_corr.show()\n",
    "            \n",
    "            # Create scatter plots for top demographic correlations\n",
    "            for i, demo_factor in enumerate(top_demos[:3]):  # Show top 3 scatter plots\n",
    "                demo_col = demo_factor.replace(' ', '_').lower()\n",
    "                if demo_col in demo_analysis.columns:# Comprehensive COVID-19 Global Data Analysis\n",
    "# Advanced analytics and insights for pandemic trends worldwide\n",
    "\n",
    "# Import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set styling for plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"Set2\")\n",
    "print(\"Setting up environment and loading packages...\")\n",
    "\n",
    "# Interactive widgets configuration (if running in notebooks)\n",
    "try:\n",
    "    from google.colab import output\n",
    "    IN_COLAB = True\n",
    "    print(\"Running in Google Colab environment\")\n",
    "    from plotly.offline import init_notebook_mode\n",
    "    init_notebook_mode(connected=True)\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"Not running in Google Colab\")\n",
    "\n",
    "print(\"\\n=== Loading and Preparing COVID-19 Data ===\")\n",
    "print(\"Loading COVID-19 data from Our World in Data...\")\n",
    "\n",
    "# Load the COVID-19 dataset\n",
    "df = pd.read_csv('https://covid.ourworldindata.org/data/owid-covid-data.csv')\n",
    "\n",
    "# Convert date column to datetime\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Print basic dataset information\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}\")\n",
    "print(f\"Number of countries/regions: {df['location'].nunique()}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "\n",
    "# Display a summary of available metrics\n",
    "metric_categories = {\n",
    "    'Cases': [col for col in df.columns if 'case' in col.lower()],\n",
    "    'Deaths': [col for col in df.columns if 'death' in col.lower()],\n",
    "    'Tests': [col for col in df.columns if 'test' in col.lower()],\n",
    "    'Vaccinations': [col for col in df.columns if 'vaccine' in col.lower() or 'vaccination' in col.lower()],\n",
    "    'Hospital & ICU': [col for col in df.columns if 'hosp' in col.lower() or 'icu' in col.lower()],\n",
    "    'Policy': [col for col in df.columns if 'stringency' in col.lower() or 'policy' in col.lower()],\n",
    "    'Demographics': ['population', 'population_density', 'median_age', 'aged_65_older', 'aged_70_older', 'gdp_per_capita', 'life_expectancy']\n",
    "}\n",
    "\n",
    "print(\"\\n=== Available Data Categories ===\")\n",
    "for category, metrics in metric_categories.items():\n",
    "    if metrics:  # Only print categories that have available metrics\n",
    "        print(f\"{category}: {len(metrics)} metrics\")\n",
    "        # Print a few example metrics from each category\n",
    "        print(f\"  Example metrics: {', '.join(metrics[:3])}\" + (\" and more...\" if len(metrics) > 3 else \"\"))\n",
    "\n",
    "# Handle missing values appropriately\n",
    "print(\"\\n=== Data Cleaning and Preparation ===\")\n",
    "print(\"Handling missing values...\")\n",
    "\n",
    "# Calculate missing values percentage for key metrics\n",
    "key_metrics = ['new_cases', 'new_deaths', 'total_cases', 'total_deaths']\n",
    "missing_stats = pd.DataFrame({\n",
    "    'Missing (%)': {col: df[col].isna().mean() * 100 for col in key_metrics}\n",
    "})\n",
    "print(\"Missing values in key metrics:\")\n",
    "print(missing_stats)\n",
    "\n",
    "# Fill missing values in key metrics\n",
    "for col in key_metrics:\n",
    "    if col in df.columns:\n",
    "        # Fill NaN values with 0 for cumulative metrics (more appropriate)\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "# Create additional analytical metrics\n",
    "print(\"\\nCreating advanced metrics...\")\n",
    "\n",
    "# Calculate rolling averages for smoother visualization\n",
    "df['cases_7day_avg'] = df.groupby('location')['new_cases'].transform(\n",
    "    lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "df['deaths_7day_avg'] = df.groupby('location')['new_deaths'].transform(\n",
    "    lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "\n",
    "# Calculate growth rates\n",
    "df['case_growth_rate'] = df.groupby('location')['total_cases'].pct_change() * 100\n",
    "df['death_growth_rate'] = df.groupby('location')['total_deaths'].pct_change() * 100\n",
    "\n",
    "# Calculate doubling rates (in days) where meaningful\n",
    "df['case_doubling_days'] = np.log(2) / (np.log(1 + df['case_growth_rate']/100))\n",
    "df['death_doubling_days'] = np.log(2) / (np.log(1 + df['death_growth_rate']/100))\n",
    "\n",
    "# Handle infinite and extreme values\n",
    "for col in ['case_doubling_days', 'death_doubling_days']:\n",
    "    # Replace infinite values with NaN\n",
    "    df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "    # Cap extreme values for better visualization\n",
    "    upper_limit = df[col].quantile(0.95)  # 95th percentile\n",
    "    df[col] = df[col].clip(upper=upper_limit)\n",
    "\n",
    "# Calculate case fatality rate (CFR)\n",
    "df['case_fatality_rate'] = np.where(\n",
    "    df['total_cases'] > 0,\n",
    "    (df['total_deaths'] / df['total_cases']) * 100,\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Create per capita metrics\n",
    "if 'population' in df.columns:\n",
    "    print(\"Calculating per capita metrics...\")\n",
    "    per_capita_metrics = {\n",
    "        'cases_per_million': 'total_cases',\n",
    "        'deaths_per_million': 'total_deaths',\n",
    "        'daily_cases_per_million': 'new_cases',\n",
    "        'daily_deaths_per_million': 'new_deaths'\n",
    "    }\n",
    "    \n",
    "    for new_col, base_col in per_capita_metrics.items():\n",
    "        if base_col in df.columns:\n",
    "            df[new_col] = df[base_col] * 1000000 / df['population']\n",
    "\n",
    "# Calculate effective reproduction number (Rt) - simplified version\n",
    "# Using the method of 7-day growth rate\n",
    "df['cases_7day_sum'] = df.groupby('location')['new_cases'].transform(\n",
    "    lambda x: x.rolling(window=7).sum())\n",
    "df['prev_cases_7day_sum'] = df.groupby('location')['cases_7day_sum'].shift(7)\n",
    "\n",
    "# Approximate Rt based on weekly growth\n",
    "# Assuming serial interval of ~5 days\n",
    "df['approx_rt'] = np.where(\n",
    "    df['prev_cases_7day_sum'] > 10,  # Only calculate where we have meaningful previous cases\n",
    "    df['cases_7day_sum'] / df['prev_cases_7day_sum'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Handle outliers in Rt\n",
    "df['approx_rt'] = df['approx_rt'].clip(0, 10)  # Cap extreme values\n",
    "\n",
    "print(\"Data preparation complete.\\n\")\n",
    "\n",
    "# ====== GLOBAL PANDEMIC OVERVIEW ======\n",
    "print(\"\\n=== GLOBAL PANDEMIC OVERVIEW ===\")\n",
    "\n",
    "# Create global time series by summing across all countries for each date\n",
    "global_df = df.groupby('date')[['new_cases', 'new_deaths', 'total_cases', 'total_deaths']].sum().reset_index()\n",
    "\n",
    "# Calculate 7-day averages for the global data\n",
    "global_df['cases_7day_avg'] = global_df['new_cases'].rolling(window=7).mean()\n",
    "global_df['deaths_7day_avg'] = global_df['new_deaths'].rolling(window=7).mean()\n",
    "\n",
    "# Plot the global pandemic overview\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=1,\n",
    "    subplot_titles=(\"Global Daily COVID-19 Cases\", \"Global Daily COVID-19 Deaths\"),\n",
    "    vertical_spacing=0.15,\n",
    "    shared_xaxes=True\n",
    ")\n",
    "\n",
    "# Add daily cases to the first subplot\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=global_df['date'], \n",
    "        y=global_df['new_cases'],\n",
    "        name=\"Daily Cases\",\n",
    "        marker_color='rgba(55, 128, 191, 0.5)'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=global_df['date'], \n",
    "        y=global_df['cases_7day_avg'],\n",
    "        name=\"7-Day Average (Cases)\",\n",
    "        line=dict(color='rgb(40, 61, 163)', width=2)\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add daily deaths to the second subplot\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=global_df['date'], \n",
    "        y=global_df['new_deaths'],\n",
    "        name=\"Daily Deaths\",\n",
    "        marker_color='rgba(219, 64, 82, 0.5)'\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=global_df['date'], \n",
    "        y=global_df['deaths_7day_avg'],\n",
    "        name=\"7-Day Average (Deaths)\",\n",
    "        line=dict(color='rgb(168, 42, 42)', width=2)\n",
    "    ),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"Global COVID-19 Pandemic Timeline\",\n",
    "    hovermode=\"x unified\",\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    ")\n",
    "\n",
    "# Update axes\n",
    "fig.update_yaxes(title_text=\"Number of Cases\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Number of Deaths\", row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"Date\", row=2, col=1)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n",
    "\n",
    "# Analyze pandemic waves globally\n",
    "print(\"\\nAnalyzing global pandemic waves...\")\n",
    "\n",
    "# For a simplified wave detection, we'll find peaks in the 7-day average cases\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# Find peaks with prominence of at least 5% of maximum value to identify significant waves\n",
    "peaks, _ = find_peaks(global_df['cases_7day_avg'], prominence=global_df['cases_7day_avg'].max() * 0.05)\n",
    "wave_dates = global_df.iloc[peaks]['date']\n",
    "wave_heights = global_df.iloc[peaks]['cases_7day_avg']\n",
    "\n",
    "print(f\"Detected {len(peaks)} major global COVID-19 waves:\")\n",
    "for i, (date, height) in enumerate(zip(wave_dates, wave_heights)):\n",
    "    print(f\"  Wave {i+1}: Peak on {date.strftime('%Y-%m-%d')} with {height:.0f} average daily cases\")\n",
    "\n",
    "# ====== REGIONAL ANALYSIS ======\n",
    "print(\"\\n=== REGIONAL ANALYSIS ===\")\n",
    "\n",
    "# Check if 'continent' column exists for regional grouping\n",
    "if 'continent' in df.columns:\n",
    "    # Group data by continent and date\n",
    "    continent_data = df.groupby(['continent', 'date'])[['new_cases', 'new_deaths']].sum().reset_index()\n",
    "    \n",
    "    # Calculate 7-day averages for each continent\n",
    "    continent_data['cases_7day_avg'] = continent_data.groupby('continent')['new_cases'].transform(\n",
    "        lambda x: x.rolling(window=7, min_periods=1).mean())\n",
    "    \n",
    "    # Visualization of cases by continent\n",
    "    fig_continent = px.line(\n",
    "        continent_data,\n",
    "        x='date',\n",
    "        y='cases_7day_avg',\n",
    "        color='continent',\n",
    "        title=\"COVID-19 Cases by Continent (7-day Average)\",\n",
    "        labels={'cases_7day_avg': '7-day Average Cases', 'date': 'Date', 'continent': 'Continent'}\n",
    "    )\n",
    "    \n",
    "    fig_continent.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"7-day Average Cases\",\n",
    "        legend_title=\"Continent\",\n",
    "        hovermode=\"x unified\"\n",
    "    )\n",
    "    \n",
    "    fig_continent.show()\n",
    "    \n",
    "    # Calculate pandemic timing differences between continents\n",
    "    pandemic_timing = continent_data.groupby('continent').apply(\n",
    "        lambda x: x.loc[x['cases_7day_avg'] > 1000, 'date'].min() if any(x['cases_7day_avg'] > 1000) else pd.NaT\n",
    "    ).reset_index()\n",
    "    pandemic_timing.columns = ['continent', 'date_reached_1000_cases']\n",
    "    \n",
    "    if not pandemic_timing['date_reached_1000_cases'].isna().all():\n",
    "        # Sort continents by when they reached 1000 daily cases\n",
    "        pandemic_timing = pandemic_timing.sort_values('date_reached_1000_cases')\n",
    "        \n",
    "        print(\"\\nPandemic progression across continents:\")\n",
    "        print(\"When each continent first reached 1,000 daily cases (7-day avg):\")\n",
    "        \n",
    "        for _, row in pandemic_timing.iterrows():\n",
    "            if pd.notna(row['date_reached_1000_cases']):\n",
    "                print(f\"  {row['continent']}: {row['date_reached_1000_cases'].strftime('%Y-%m-%d')}\")\n",
    "            else:\n",
    "                print(f\"  {row['continent']}: Never reached 1,000 daily cases\")\n",
    "else:\n",
    "    print(\"Continent data not available in the dataset.\")\n",
    "\n",
    "# ====== COUNTRY COMPARISON ANALYSIS ======\n",
    "print(\"\\n=== COUNTRY COMPARISON ANALYSIS ===\")\n",
    "\n",
    "# Find the top countries with most cases\n",
    "latest_date = df['date'].max()\n",
    "latest = df[df['date'] == latest_date]\n",
    "\n",
    "# Get top 15 countries by total cases\n",
    "top_countries = latest.sort_values('total_cases', ascending=False).head(15)['location'].tolist()\n",
    "print(f\"Top countries by total cases: {', '.join(top_countries[:5])}, and more...\")\n",
    "\n",
    "# Filter data for these top countries\n",
    "top_countries_data = df[df['location'].isin(top_countries)].copy()\n",
    "\n",
    "# Prepare data for comparison - normalize by population\n",
    "if 'population' in top_countries_data.columns:\n",
    "    # Ensure we have per capita metrics for comparison\n",
    "    if 'cases_per_million' not in top_countries_data.columns:\n",
    "        top_countries_data['cases_per_million'] = top_countries_data['total_cases'] * 1000000 / top_countries_data['population']\n",
    "    if 'deaths_per_million' not in top_countries_data.columns:\n",
    "        top_countries_data['deaths_per_million'] = top_countries_data['total_deaths'] * 1000000 / top_countries_data['population']\n",
    "\n",
    "    # Create comparative visualization for cases per million\n",
    "    fig_cases_per_million = px.line(\n",
    "        top_countries_data,\n",
    "        x='date',\n",
    "        y='cases_per_million',\n",
    "        color='location',\n",
    "        title=\"COVID-19 Cases per Million Population (Top 15 Countries)\",\n",
    "        labels={'cases_per_million': 'Cases per Million', 'date': 'Date', 'location': 'Country'}\n",
    "    )\n",
    "    \n",
    "    fig_cases_per_million.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Cases per Million\",\n",
    "        legend_title=\"Country\",\n",
    "        hovermode=\"x unified\"\n",
    "    )\n",
    "    \n",
    "    fig_cases_per_million.show()\n",
    "    \n",
    "    # Create comparative visualization for deaths per million\n",
    "    fig_deaths_per_million = px.line(\n",
    "        top_countries_data,\n",
    "        x='date',\n",
    "        y='deaths_per_million',\n",
    "        color='location',\n",
    "        title=\"COVID-19 Deaths per Million Population (Top 15 Countries)\",\n",
    "        labels={'deaths_per_million': 'Deaths per Million', 'date': 'Date', 'location': 'Country'}\n",
    "    )\n",
    "    \n",
    "    fig_deaths_per_million.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Deaths per Million\",\n",
    "        legend_title=\"Country\",\n",
    "        hovermode=\"x unified\"\n",
    "    )\n",
    "    \n",
    "    fig_deaths_per_million.show()\n",
    "\n",
    "# Calculate the pandemic severity index (combination of case and death rates)\n",
    "if all(col in latest.columns for col in ['cases_per_million', 'deaths_per_million']):\n",
    "    # Normalize to 0-100 scale\n",
    "    latest['cases_score'] = 100 * latest['cases_per_million'] / latest['cases_per_million'].max()\n",
    "    latest['deaths_score'] = 100 * latest['deaths_per_million'] / latest['deaths_per_million'].max()\n",
    "    \n",
    "    # Calculate combined severity score (higher weight on deaths)\n",
    "    latest['severity_index'] = (0.4 * latest['cases_score'] + 0.6 * latest['deaths_score'])\n",
    "    \n",
    "    # Filter for countries with significant case counts\n",
    "    severity_data = latest[latest['total_cases'] > 10000].sort_values('severity_index', ascending=False)\n",
    "    \n",
    "    # Plot top 20 countries by severity index\n",
    "    top_severity = severity_data.head(20)\n",
    "    fig_severity = px.bar(\n",
    "        top_severity,\n",
    "        x='location',\n",
    "        y='severity_index',\n",
    "        title=\"COVID-19 Pandemic Severity Index (Top 20 Countries)\",\n",
    "        color='severity_index',\n",
    "        labels={'severity_index': 'Severity Index', 'location': 'Country'},\n",
    "        color_continuous_scale=px.colors.sequential.Plasma\n",
    "    )\n",
    "    \n",
    "    fig_severity.update_layout(\n",
    "        xaxis_title=\"Country\",\n",
    "        yaxis_title=\"Severity Index (0-100)\",\n",
    "        xaxis={'categoryorder':'total descending'}\n",
    "    )\n",
    "    \n",
    "    fig_severity.show()\n",
    "    \n",
    "    print(\"\\nTop 10 countries by pandemic severity index:\")\n",
    "    for i, (_, row) in enumerate(top_severity[:10].iterrows()):\n",
    "        print(f\"  {i+1}. {row['location']}: {row['severity_index']:.1f} (Cases per million: {row['cases_per_million']:.0f}, Deaths per million: {row['deaths_per_million']:.0f})\")\n",
    "\n",
    "# ====== CASE FATALITY RATE ANALYSIS ======\n",
    "print(\"\\n=== CASE FATALITY RATE (CFR) ANALYSIS ===\")\n",
    "\n",
    "# Filter for countries with significant case counts for more reliable CFR\n",
    "cfr_data = latest[(latest['total_cases'] >= 10000) & (~latest['case_fatality_rate'].isna())]\n",
    "\n",
    "if not cfr_data.empty:\n",
    "    # Sort by CFR\n",
    "    cfr_data_sorted = cfr_data.sort_values('case_fatality_rate', ascending=False)\n",
    "    \n",
    "    # Plot top and bottom countries by CFR\n",
    "    top_cfr = cfr_data_sorted.head(10)\n",
    "    bottom_cfr = cfr_data_sorted.tail(10)\n",
    "    \n",
    "    # Create a figure with two subplots\n",
    "    fig_cfr = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=(\"Highest Case Fatality Rates\", \"Lowest Case Fatality Rates\"),\n",
    "        specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}]],\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    # Add top CFR countries\n",
    "    fig_cfr.add_trace(\n",
    "        go.Bar(\n",
    "            x=top_cfr['location'],\n",
    "            y=top_cfr['case_fatality_rate'],\n",
    "            marker_color='darkred',\n",
    "            name=\"Highest CFR\"\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Add bottom CFR countries\n",
    "    fig_cfr.add_trace(\n",
    "        go.Bar(\n",
    "            x=bottom_cfr['location'],\n",
    "            y=bottom_cfr['case_fatality_rate'],\n",
    "            marker_color='darkgreen',\n",
    "            name=\"Lowest CFR\"\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "    \n",
    "    # Update layout\n",
    "    fig_cfr.update_layout(\n",
    "        title_text=\"Countries with Highest and Lowest Case Fatality Rates (minimum 10,000 cases)\",\n",
    "        height=500,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    # Update y-axes\n",
    "    fig_cfr.update_yaxes(title_text=\"Case Fatality Rate (%)\", row=1, col=1)\n",
    "    fig_cfr.update_yaxes(title_text=\"Case Fatality Rate (%)\", row=1, col=2)\n",
    "    \n",
    "    # Show the figure\n",
    "    fig_cfr.show()\n",
    "    \n",
    "    # Calculate global average CFR\n",
    "    global_cfr = np.sum(cfr_data['total_deaths']) / np.sum(cfr_data['total_cases']) * 100\n",
    "    median_cfr = cfr_data['case_fatality_rate'].median()\n",
    "    \n",
    "    print(f\"\\nGlobal case fatality rate: {global_cfr:.2f}%\")\n",
    "    print(f\"Median country case fatality rate: {median_cfr:.2f}%\")\n",
    "    \n",
    "    # Analyze factors potentially related to CFR\n",
    "    if all(col in cfr_data.columns for col in ['median_age', 'case_fatality_rate']):\n",
    "        corr = cfr_data['median_age'].corr(cfr_data['case_fatality_rate'])\n",
    "        print(f\"\\nCorrelation between median age and CFR: {corr:.2f}\")\n",
    "        \n",
    "        # Create a scatter plot\n",
    "        fig_age_cfr = px.scatter(\n",
    "            cfr_data,\n",
    "            x='median_age',\n",
    "            y='case_fatality_rate',\n",
    "            hover_name='location',\n",
    "            title=\"Relationship Between Country Median Age and COVID-19 Case Fatality Rate\",\n",
    "            trendline=\"ols\",\n",
    "            labels={'median_age': 'Median Age', 'case_fatality_rate': 'Case Fatality Rate (%)'}\n",
    "        )\n",
    "        \n",
    "        fig_age_cfr.update_layout(\n",
    "            xaxis_title=\"Median Age\",\n",
    "            yaxis_title=\"Case Fatality Rate (%)\"\n",
    "        )\n",
    "        \n",
    "        fig_age_cfr.show()\n",
    "\n",
    "# ====== VACCINATION ANALYSIS ======\n",
    "print(\"\\n=== VACCINATION ANALYSIS ===\")\n",
    "\n",
    "# Check if vaccination data is available\n",
    "vax_columns = [col for col in df.columns if 'vaccine' in col.lower() or 'vaccination' in col.lower()]\n",
    "\n",
    "if vax_columns:\n",
    "    print(f\"Vaccination data available: {', '.join(vax_columns[:5])}\" + (\" and more...\" if len(vax_columns) > 5 else \"\"))\n",
    "    \n",
    "    # Focus on fully vaccinated population\n",
    "    if 'people_fully_vaccinated_per_hundred' in df.columns:\n",
    "        # Get latest vaccination rates\n",
    "        vax_data = df.dropna(subset=['people_fully_vaccinated_per_hundred'])\n",
    "        if not vax_data.empty:\n",
    "            # Get the latest data point for each country\n",
    "            latest_vax = vax_data.sort_values('date').groupby('location').last().reset_index()\n",
    "            \n",
    "            # Plot top vaccination rates\n",
    "            top_vax = latest_vax.sort_values('people_fully_vaccinated_per_hundred', ascending=False).head(20)\n",
    "            fig_vax = px.bar(\n",
    "                top_vax,\n",
    "                x='location',\n",
    "                y='people_fully_vaccinated_per_hundred',\n",
    "                title=\"Top 20 Countries by Full Vaccination Rate\",\n",
    "                color='people_fully_vaccinated_per_hundred',\n",
    "                labels={'people_fully_vaccinated_per_hundred': 'Population Fully Vaccinated (%)', 'location': 'Country'},\n",
    "                color_continuous_scale=px.colors.sequential.Viridis\n",
    "            )\n",
    "            \n",
    "            fig_vax.update_layout(\n",
    "                xaxis_title=\"Country\",\n",
    "                yaxis_title=\"Population Fully Vaccinated (%)\",\n",
    "                xaxis={'categoryorder':'total descending'}\n",
    "            )\n",
    "            \n",
    "            fig_vax.show()\n",
    "            \n",
    "            # Vaccination progress over time for top countries\n",
    "            # Select a few major countries\n",
    "            major_countries = ['United States', 'United Kingdom', 'Israel', 'India', 'Brazil', 'South Africa']\n",
    "            major_vax_data = df[df['location'].isin(major_countries)].dropna(subset=['people_fully_vaccinated_per_hundred'])\n",
    "            \n",
    "            if not major_vax_data.empty:\n",
    "                fig_vax_time = px.line(\n",
    "                    major_vax_data,\n",
    "                    x='date',\n",
    "                    y='people_fully_vaccinated_per_hundred',\n",
    "                    color='location',\n",
    "                    title=\"Vaccination Progress Over Time (Selected Countries)\",\n",
    "                    labels={'people_fully_vaccinated_per_hundred': 'Population Fully Vaccinated (%)', 'date': 'Date', 'location': 'Country'}\n",
    "                )\n",
    "                \n",
    "                fig_vax_time.update_layout(\n",
    "                    xaxis_title=\"Date\",\n",
    "                    yaxis_title=\"Population Fully Vaccinated (%)\",\n",
    "                    legend_title=\"Country\",\n",
    "                    hovermode=\"x unified\"\n",
    "                )\n",
    "                \n",
    "                fig_vax_time.show()\n",
    "            \n",
    "            # Analyze relationship between vaccination and recent cases\n",
    "            if 'new_cases_per_million' in latest_vax.columns:\n",
    "                # Calculate correlation between vaccination rates and recent cases\n",
    "                vax_case_corr = latest_vax[['people_fully_vaccinated_per_hundred', 'new_cases_per_million']].corr().iloc[0, 1]\n",
    "                \n",
    "                print(f\"\\nCorrelation between vaccination rates and recent cases: {vax_case_corr:.2f}\")\n",
    "                \n",
    "                # Plot relationship\n",
    "                fig_vax_cases = px.scatter(\n",
    "                    latest_vax,\n",
    "                    x='people_fully_vaccinated_per_hundred',\n",
    "                    y='new_cases_per_million',\n",
    "                    hover_name='location',\n",
    "                    title=\"Relationship Between Vaccination Rate and Recent COVID-19 Cases\",\n",
    "                    trendline=\"ols\",\n",
    "                    labels={'people_fully_vaccinated_per_hundred': 'Population Fully Vaccinated (%)', \n",
    "                           'new_cases_per_million': 'New Cases per Million'}\n",
    "                )\n",
    "                \n",
    "                fig_vax_cases.update_layout(\n",
    "                    xaxis_title=\"Population Fully Vaccinated (%)\",\n",
    "                    yaxis_title=\"New Cases per Million\"\n",
    "                )\n",
    "                \n",
    "                fig_vax_cases.show()\n",
    "        else:\n",
    "            print(\"Not enough data points for vaccination analysis.\")\n",
    "    else:\n",
    "        print(\"Full vaccination data not available in this dataset.\")\n",
    "else:\n",
    "    print(\"Vaccination data not available in this dataset.\")\n",
    "\n",
    "# ====== POLICY RESPONSE ANALYSIS ======\n",
    "print(\"\\n=== POLICY RESPONSE ANALYSIS ===\")\n",
    "\n",
    "# Check for policy strictness data\n",
    "if 'stringency_index' in df.columns:\n",
    "    # Get mean stringency over time for top affected countries\n",
    "    stringency_data = df[df['location'].isin(top_countries)].copy()\n",
    "    \n",
    "    # Plot stringency index over time\n",
    "    fig_stringency = px.line(\n",
    "        stringency_data,\n",
    "        x='date',\n",
    "        y='stringency_index',\n",
    "        color='location',\n",
    "        title=\"COVID-19 Policy Stringency Index Over Time (Top Countries)\",\n",
    "        labels={'stringency_index': 'Stringency Index (0-100)', 'date': 'Date', 'location': 'Country'}\n",
    "    )\n",
    "    \n",
    "    fig_stringency.update_layout(\n",
    "        xaxis_title=\"Date\",\n",
    "        yaxis_title=\"Stringency Index (0-100)\",\n",
    "        legend_title=\"Country\",\n",
    "        hovermode=\"x unified\"\n",
    "    )\n",
    "    \n",
    "    fig_stringency.show()\n",
    "    \n",
    "    # Analyze relationship between stringency and case growth\n",
    "    # Calculate average stringency for the past 3 months\n",
    "    three_months_ago = latest_date - pd.Timedelta(days=90)\n",
    "    recent_data = df[df['date'] >= three_months_ago].copy()\n",
    "    \n",
    "    # Group by country and calculate mean stringency and case growth\n",
    "    policy_effectiveness = recent_data.groupby('location').agg({\n",
    "        'stringency_index': 'mean',\n",
    "        'new_cases_per_million': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Filter for countries with data\n",
    "    policy_effectiveness = policy_effectiveness.dropna()\n",
    "    \n",
    "    if not policy_effectiveness.empty:\n",
    "        # Create visualization\n",
    "        fig_policy = px.scatter(\n",
    "            policy_effectiveness,\n",
    "            x='stringency_index',\n",
    "            y='new_cases_per_million',\n",
    "            hover_name='location',\n",
    "            title=\"Relationship Between Policy Stringency and COVID-19 Cases (Last 3 Months)\",\n",
    "            trendline=\"ols\",\n",
    "            labels={'stringency_index': 'Average Stringency Index (0-100)', \n",
    "                   'new_cases_per_million': 'Average Daily Cases per Million'}\n",
    "        )\n",
    "        \n",
    "        fig_policy.update_layout(\n",
    "            xaxis_title=\"Average Stringency Index (0-100)\",\n",
    "            yaxis_title=\"Average Daily Cases per Million\"\n",
    "        )\n",
    "        \n",
    "        fig_policy.show()\n",
    "        \n",
    "        # Calculate correlation\n",
    "        policy_corr = policy_effectiveness['stringency_index'].corr(policy_effectiveness['new_cases_per_million'])\n",
    "        print(f\"Correlation between policy stringency and recent cases: {policy_corr:.2f}\")\n",
    "        \n",
    "        # Analysis of the result\n",
    "        if policy_corr < -0.3:\n",
    "            print(\"There appears to be a negative correlation between policy stringency and cases,\")\n",
    "            print(\"suggesting stricter policies may help reduce case counts.\")\n",
    "        elif policy_corr > 0.3:\n",
    "            print(\"There appears to be a positive correlation between policy stringency and cases,\")\n",
    "            print(\"which might indicate that countries implement stricter policies in response to rising cases.\")\n",
    "        else:\n",
    "            print(\"There's no strong correlation between policy stringency and cases,\")\n",
    "            print(\"suggesting complex relationships between policies and outcomes.\")\n",
    "else:\n",
    "    print(\"Policy stringency data not available in this dataset.\")\n",
    "\n",
    "# ====== ADVANCED STATISTICAL ANALYSIS ======\n",
    "print(\"\\n=== ADVANCED STATISTICAL ANALYSIS ===\")\n",
    "\n",
    "# Select a few countries for detailed analysis\n",
    "sample_countries = ['United States', 'Germany', 'Brazil', 'India', 'South Korea']\n",
    "countries_present = [country for country in sample_countries if country in df['location'].unique()]\n",
    "\n",
    "if countries_present:\n",
    "    print(f\"Performing detailed analysis for: {', '.join(countries_present)}\")\n",
    "    \n",
    "    for country in countries_present:\n",
    "        print(f\"\\nDetailed analysis for {country}:\")\n",
    "        \n",
    "        # Extract country data and ensure it has enough data points\n",
    "        country_data = df[df['location'] == country].sort_values('date')\n",
    "        \n",
    "        if len(country_data) < 30:\n",
    "            print(f\"  Not enough data points for {country}\")\n",
    "            continue\n",
    "        \n",
    "        # Basic statistics for the country\n",
    "        total_cases = country_data['total_cases'].max()\n",
    "        total_deaths = country_data['total_deaths'].max()\n",
    "        cfr = (total_deaths / total_cases * 100) if total_cases > 0 else 0\n",
    "        \n",
    "        print(f\"  Total Cases: {total_cases:,.0f}\")\n",
    "        print(f\"  Total Deaths: {total_deaths:,.0f}\")\n",
    "        print(f\"  Case Fatality Rate: {cfr:.2f}%\")\n",
    "        \n",
    "        # Time series decomposition of daily cases\n",
    "        if len(country_data) >= 90:  # Need enough data for decomposition\n",
    "            # Get the daily cases data and fill missing values for time series analysis\n",
    "            ts_data = country_data['new_cases'].fillna(0)\n",
    "            \n",
    "            if np.sum(ts_data) > 0:  # Ensure we have non-zero data\n",
    "                # Use 7-day rolling average to smooth out reporting anomalies\n",
    "                ts_data_smoothed = ts_data.rolling(window=7, min_periods=1).mean()\n",
    "                \n",
    "                # Perform time series decomposition\n",
    "                try:\n",
    "                    result = seasonal_decompose(ts_data_smoothed, model='additive', period=7)\n",
    "                    \n",
    "                    # Create figure for decomposition\n",
    "                    fig_decomp = make_subplots(\n",
    "                        rows=4, cols=1,\n",
    "                        subplot_titles=(\"Observed\", \"Trend\", \"Seasonal\", \"Residual\"),\n",
    "                        vertical_spacing=0.1,\n",
    "                        shared_xaxes=True\n",
    "                    )\n",
    "                    \n",
    "                    # Add observed data\n",
    "                    fig_decomp.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=country_data['date'], \n",
    "                            y=result.observed,\n",
    "                            name=\"Observed\",\n",
    "                            line=dict(color='blue')\n",
    "                        ),\n",
    "                        row=1, col=1\n",
    "                    )\n",
    "                    \n",
    "                    # Add trend component\n",
    "                    fig_decomp.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=country_data['date'], \n",
    "                            y=result.trend,\n",
    "                            name=\"Trend\",\n",
    "                            line=dict(color='red')\n",
    "                        ),\n",
    "                        row=2, col=1\n",
    "                    )\n",
    "                    \n",
    "                    # Add seasonal component\n",
    "                    fig_decomp.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=country_data['date'], \n",
    "                            y=result.seasonal,\n",
    "                            name=\"Seasonal\",\n",
    "                            line=dict(color='green')\n",
    "                        ),\n",
    "                        row=3, col=1\n",
    "                    )\n",
    "                    \n",
    "                    # Add residual component\n",
    "                    fig_decomp.add_trace(\n",
    "                        go.Scatter(\n",
    "                            x=country_data['date'], \n",
    "                            y=result.resid,\n",
    "                            name=\"Residual\",\n",
    "                            line=dict(color='purple')\n",
    "                        ),\n",
    "                        row=4, col=1\n",
    "                    )\n",
    "                    \n",
    "                    # Update layout\n",
    "                    fig_decomp.update_layout(\n",
    "                        height=800,\n",
    "                        title_text=f\"Time Series Decomposition of COVID-19 Cases in {country}\",\n",
    "                        showlegend=False\n",
    "                    )\n",
    "                    \n",
    "                    fig_decomp.show()\n",
    "                    \n",
    "                    # Calculate statistics about seasonality\n",
    "                    day_of_week_effect = np.abs(result.seasonal[:7].mean())\n",
    "                    print(f\"  Day-of-week reporting effect magnitude: {day_of_week_effect:.2f} cases\")\n",
    "                    \n",
    "                    # Check for trend direction in recent data (last 30 days)\n",
    "                    recent_trend = result.trend[-30:].dropna()\n",
    "                    if len(recent_trend) > 0:\n",
    "                        trend_direction = recent_trend.iloc[-1] - recent_trend.iloc[0]\n",
    "                        if trend_direction > 0:\n",
    "                            print(f\"  Recent trend: INCREASING by {trend_direction:.2f} cases over the last 30 days\")\n",
    "                        else:\n",
    "                            print(f\"  Recent trend: DECREASING by {abs(trend_direction):.2f} cases over the last 30 days\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    print(f\"  Could not perform time series decomposition: {e}\")\n",
    "        \n",
    "        # Analyze effective reproduction number (Rt) over time if available\n",
    "        if 'approx_rt' in country_data.columns:\n",
    "            valid_rt = country_data.dropna(subset=['approx_rt'])\n",
    "            \n",
    "            if len(valid_rt) > 0:\n",
    "                # Calculate statistics\n",
    "                current_rt = valid_rt['approx_rt'].iloc[-1]\n",
    "                mean_rt = valid_rt['approx_rt'].mean()\n",
    "                max_rt = valid_rt['approx_rt'].max()\n",
    "                \n",
    "                print(f\"  Current effective reproduction number (Rt): {current_rt:.2f}\")\n",
    "                print(f\"  Average Rt throughout pandemic: {mean_rt:.2f}\")\n",
    "                print(f\"  Maximum Rt recorded: {max_rt:.2f}\")\n",
    "                \n",
    "                # Determine pandemic control status\n",
    "                if current_rt < 1:\n",
    "                    control_status = \"CONTROLLED (Rt < 1)\"\n",
    "                else:\n",
    "                    control_status = \"GROWING (Rt > 1)\"\n",
    "                \n",
    "                print(f\"  Current pandemic status: {control_status}\")\n",
    "                \n",
    "                # Plot Rt over time\n",
    "                fig_rt = px.line(\n",
    "                    valid_rt,\n",
    "                    x='date',\n",
    "                    y='approx_rt',\n",
    "                    title=f\"Effective Reproduction Number (Rt) Over Time in {country}\",\n",
    "                    labels={'approx_rt': 'Effective Reproduction Number (Rt)', 'date': 'Date'}\n",
    "                )\n",
    "                \n",
    "                # Add reference line at Rt = 1\n",
    "                fig_rt.add_hline(\n",
    "                    y=1, \n",
    "                    line_dash=\"dash\", \n",
    "                    line_color=\"red\",\n",
    "                    annotation_text=\"Rt = 1 (Control Threshold)\",\n",
    "                    annotation_position=\"bottom right\"\n",
    "                )\n",
    "                \n",
    "                fig_rt.update_layout(\n",
    "                    xaxis_title=\"Date\",\n",
    "                    yaxis_title=\"Effective Reproduction Number (Rt)\"\n",
    "                )\n",
    "                \n",
    "                fig_rt.show()\n",
    "        \n",
    "        # Calculate and visualize pandemic waves for this country\n",
    "        if 'cases_7day_avg' in country_data.columns:\n",
    "            # Use the 7-day average to find significant peaks (waves)\n",
    "            smooth_cases = country_data['cases_7day_avg'].fillna(0).values\n",
    "            \n",
    "            # Find peaks with prominence of at least 10% of the maximum\n",
    "            peaks, properties = find_peaks(\n",
    "                smooth_cases, \n",
    "                prominence=np.max(smooth_cases) * 0.1,\n",
    "                distance=14  # Minimum 2 weeks between peaks\n",
    "            )\n",
    "            \n",
    "            if len(peaks) > 0:\n",
    "                print(f\"  Detected {len(peaks)} major COVID-19 waves:\")\n",
    "                \n",
    "                wave_data = []\n",
    "                for i, peak_idx in enumerate(peaks):\n",
    "                    peak_date = country_data.iloc[peak_idx]['date']\n",
    "                    peak_cases = smooth_cases[peak_idx]\n",
    "                    prominence = properties['prominences'][i]\n",
    "                    \n",
    "                    wave_data.append({\n",
    "                        'Wave': i+1,\n",
    "                        'Date': peak_date,\n",
    "                        'Cases': peak_cases,\n",
    "                        'Prominence': prominence\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"    Wave {i+1}: Peak on {peak_date.strftime('%Y-%m-%d')} with {peak_cases:.0f} avg daily cases (prominence: {prominence:.0f})\")\n",
    "                \n",
    "                # Create a visualization of the waves\n",
    "                fig_waves = go.Figure()\n",
    "                \n",
    "                # Add the case curve\n",
    "                fig_waves.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=country_data['date'],\n",
    "                        y=country_data['cases_7day_avg'],\n",
    "                        name=\"7-day Avg Cases\",\n",
    "                        line=dict(color='blue', width=1)\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                # Add markers for each wave peak\n",
    "                fig_waves.add_trace(\n",
    "                    go.Scatter(\n",
    "                        x=[country_data.iloc[peak_idx]['date'] for peak_idx in peaks],\n",
    "                        y=[smooth_cases[peak_idx] for peak_idx in peaks],\n",
    "                        mode='markers+text',\n",
    "                        marker=dict(color='red', size=10),\n",
    "                        text=[f\"Wave {i+1}\" for i in range(len(peaks))],\n",
    "                        textposition=\"top center\",\n",
    "                        name=\"Wave Peaks\"\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                fig_waves.update_layout(\n",
    "                    title=f\"COVID-19 Waves in {country}\",\n",
    "                    xaxis_title=\"Date\",\n",
    "                    yaxis_title=\"7-day Average Cases\",\n",
    "                    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    "                )\n",
    "                \n",
    "                fig_waves.show()\n",
    "                \n",
    "                # Analyze wave characteristics\n",
    "                if len(wave_data) > 1:\n",
    "                    wave_intervals = []\n",
    "                    for i in range(1, len(wave_data)):\n",
    "                        interval = (wave_data[i]['Date'] - wave_data[i-1]['Date']).days\n",
    "                        wave_intervals.append(interval)\n",
    "                    \n",
    "                    avg_interval = np.mean(wave_intervals)\n",
    "                    print(f\"  Average time between waves: {avg_interval:.0f} days\")\n",
    "                    \n",
    "                    # Analyze wave severity progression\n",
    "                    first_wave_size = wave_data[0]['Cases']\n",
    "                    last_wave_size = wave_data[-1]['Cases']\n",
    "                    \n",
    "                    if last_wave_size > first_wave_size:\n",
    "                        print(f\"  Wave progression: INCREASING in severity (Last/First ratio: {last_wave_size/first_wave_size:.2f}x)\")\n",
    "                    else:\n",
    "                        print(f\"  Wave progression: DECREASING in severity (Last/First ratio: {last_wave_size/first_wave_size:.2f}x)\")\n",
    "        \n",
    "        # Analyze relationship between testing and case detection if data available\n",
    "        if all(col in country_data.columns for col in ['new_tests', 'new_cases']):\n",
    "            valid_test_data = country_data.dropna(subset=['new_tests', 'new_cases'])\n",
    "            \n",
    "            if len(valid_test_data) > 30:  # Ensure we have enough data points\n",
    "                # Calculate positivity rate\n",
    "                valid_test_data['positivity_rate'] = (valid_test_data['new_cases'] / valid_test_data['new_tests']) * 100\n",
    "                \n",
    "                # Visualize positivity rate over time\n",
    "                fig_pos = px.line(\n",
    "                    valid_test_data,\n",
    "                    x='date',\n",
    "                    y='positivity_rate',\n",
    "                    title=f\"COVID-19 Test Positivity Rate in {country}\",\n",
    "                    labels={'positivity_rate': 'Positivity Rate (%)', 'date': 'Date'}\n",
    "                )\n",
    "                \n",
    "                # Add WHO reference line at 5% positivity\n",
    "                fig_pos.add_hline(\n",
    "                    y=5, \n",
    "                    line_dash=\"dash\", \n",
    "                    line_color=\"red\",\n",
    "                    annotation_text=\"WHO 5% Threshold\",\n",
    "                    annotation_position=\"bottom right\"\n",
    "                )\n",
    "                \n",
    "                fig_pos.update_layout(\n",
    "                    xaxis_title=\"Date\",\n",
    "                    yaxis_title=\"Test Positivity Rate (%)\"\n",
    "                )\n",
    "                \n",
    "                fig_pos.show()\n",
    "                \n",
    "                # Calculate statistics\n",
    "                current_pos = valid_test_data['positivity_rate'].iloc[-1]\n",
    "                avg_pos = valid_test_data['positivity_rate'].mean()\n",
    "                max_pos = valid_test_data['positivity_rate'].max()\n",
    "                \n",
    "                print(f\"  Current test positivity rate: {current_pos:.2f}%\")\n",
    "                print(f\"  Average positivity rate: {avg_pos:.2f}%\")\n",
    "                print(f\"  Maximum positivity rate: {max_pos:.2f}%\")\n",
    "                \n",
    "                # Interpret testing adequacy\n",
    "                if current_pos < 5:\n",
    "                    print(\"  Testing adequacy: SUFFICIENT (below WHO 5% threshold)\")\n",
    "                else:\n",
    "                    print(\"  Testing adequacy: INSUFFICIENT (above WHO 5% threshold)\")\n",
    "        \n",
    "        print(\"\\n  --- End of detailed analysis ---\")\n",
    "\n",
    "        \n",
    "        #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e739df2e-23f3-4181-a850-c22dcfe19c2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
